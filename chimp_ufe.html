<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>ChimpUFE – Universal Chimpanzee Face Embedder</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- Google Font -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;700&display=swap" rel="stylesheet">
    <!-- External stylesheet used by the original template (optional) -->
    <link rel="stylesheet" href="css/style_phd_times.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <script>
        /* Hide any open .popup when the user clicks / taps elsewhere */
        function closeOpenPopups(event) {
            // Element that currently has focus (could be <a class="popup_link">)
            const active = document.activeElement;

            /* If something inside a .popup was tapped, do nothing.
               Otherwise, remove focus from the trigger so :focus-within ends. */
            if (
                active &&                       // there *is* a focused element
                active.classList.contains('popup_link') &&      // and it’s a trigger
                !event.target.closest('.popup')                 // but the tap was outside
            ) {
                active.blur();                 // clears focus ⇒ CSS rule no longer matches
            }
        }

        document.addEventListener('click', closeOpenPopups, false);
        document.addEventListener('touchstart', closeOpenPopups, false); // mobile
    </script>

    <style>
        .our_framework {
            margin: auto;
            max-width: 60em;
            text-align: left;
        }



        body {
            -webkit-text-size-adjust: none;
            -moz-text-size-adjust: none;
            -ms-text-size-adjust: none;
        }

        :root {
            font-size: 5px;
        }

        @media (min-width: 320px) and (max-width: 1081.6px) {
            :root {
                font-size: -webkit-calc(5px + (16 - 5) * ((100vw - 320px) / (1081.6 - 320)));
                font-size: -moz-calc(5px + (16 - 5) * ((100vw - 320px) / (1081.6 - 320)));
                font-size: -o-calc(5px + (16 - 5) * ((100vw - 320px) / (1081.6 - 320)));
                font-size: calc(5px + (16 - 5) * ((100vw - 320px) / (1081.6 - 320)));
            }
        }

        @media (min-width: 1081.6px) {
            :root {
                font-size: 16px;
            }
        }

        a.intext:link,
        a.social:link,
        a.bread_crumb:link {
            border-bottom-width: 0.05em;
        }

        table {
            border-spacing: 0 5px;
            margin-left: auto;
            margin-right: auto;
            border-top: 0.15em solid black;
            border-bottom: 0.15em solid black;
            margin-bottom: 0.15em;
            border-collapse: collapse;
        }

        thead {
            border-bottom: 0.1em solid black;
            border-spacing: 5px 5px;
        }

        th {
            border-spacing: 5px 5px;
        }

        tr.mid_rule {
            border-bottom: 0.1em solid black;
        }

        th.left_border,
        td.left_border {
            border-left: 0.1em solid black;
        }

        th,
        td {
            padding: 0.4em 1em 0.4em 1em;
            text-align: right;
        }

        .rotate {
            -ms-writing-mode: tb-rl;
            -webkit-writing-mode: vertical-rl;
            writing-mode: vertical-rl;
            transform: rotate(180deg);
            white-space: nowrap;
        }



        b,
        strong {
            font-weight: 700;
        }

        details {
            border: 0.15em solid black;
            padding: 0.5em;
        }

        summary {
            cursor: pointer;
        }

        /* Popup */
        .popup_link {
            color: black;
            border-bottom: 0.1em dotted;
            color: #333;
        }

        .popup {
            position: relative;
            display: inline-block;
        }

        .popup_content {
            text-align: left;
            display: none;
            position: absolute;
            background-color: white;
            border: 0.075em solid lightgray;
            padding: 1em;
            border-radius: 1em;
            box-shadow: 0px 8px 16px 0px rgba(0, 0, 0, 0.2);
            z-index: 1;
            width: 20em;
            left: 50%;
            transform: translateX(-50%);
        }

        .popup:hover .popup_content,
        .popup:focus-within .popup_content {
            display: block;
        }

        /* hopefully helps with hovers on touch screens */
        .popup:active .popup_content {
            display: block;
        }

        .section_name {
            padding-top: 2.5em;
            padding-bottom: 0.5em;
        }

        /* ToC */
        ul {
            padding-left: 2em;
        }

        li {
            display: list-item;
            line-height: 1.8em;
        }

        p {
            font-size: 1em;
            line-height: 1.8em;
        }
    </style>

</head>

<body>
    <!-- Title -->
    <h1 style="padding: 1em 0 1em;">Self-supervised Learning on Camera Trap Footage <br> Yields a Strong Universal Face Embedder</h1>

    <!-- Authors -->
    <div class="container_authors">
        <div class="author_affiliation">
            <div class="author_name"><a class="social" href="https://v-iashin.github.io/">Vladimir Iashin</a></div>
        </div>
        <div class="author_affiliation">
            <div class="author_name"><a class="social" href="https://uk.linkedin.com/in/horace-lee-9615b1121">Horace Lee</a></div>
        </div>
        <div class="author_affiliation">
            <div class="author_name"><a class="social" href="https://www.linkedin.com/in/dan-schofield-361285239/">Dan Schofield</a></div>
        </div>
        <div class="author_affiliation">
            <div class="author_name"><a class="social" href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>
            </div>
        </div>
    </div>
    <div class="conference" style="text-align:center; font-size: larger; padding-top:0.8em;">
        Visual Geometry Group (VGG), University of Oxford
    </div>

    <!-- Conference -->
    <div class="conference" style="text-align:center; font-size: larger; padding-top:0.8em;">
        Computer Vision for Ecological and Biodiversity Monitoring Workshop, ICIP, 2025
    </div>

    <!-- Links -->
    <div class="code_and_links" style="padding-top:1em;">
        <div class="div_link"><a class="social" href="https://www.robots.ox.ac.uk/~vgg/publications/2025/Iashin25a/iashin25a.pdf">Paper</a></div>
        <div class="div_link"><a class="social" href="https://github.com/v-iashin/ChimpUFE">Code & Models</a></div>
    </div>

    <!-- Abstract -->
    <div class="our_framework">
        <div class="section_name">Abstract</div>
        <div class="section_content">
            <p>
                Camera traps are revolutionising wildlife monitoring by capturing vast amounts of visual data; however,
                the manual
                identification of individual animals remains a significant bottleneck. This study introduces a fully
                self-supervised approach to learning robust <strong>chimpanzee face embeddings</strong> from unlabelled
                camera-trap footage.
                Leveraging the <i>DINOv2</i>
                framework, we train Vision Transformers on automatically mined face crops, eliminating the need for
                identity labels. Our method demonstrates strong <em>open-set</em> re-identification performance,
                surpassing supervised baselines on challenging benchmarks such as Bossou, despite utilising no labelled
                data during training. This work underscores the potential of self-supervised learning in biodiversity
                monitoring and paves the way for scalable, non-invasive population studies.
            </p>
        </div>
    </div>

    <!-- Face Recognition -->
    <div class="our_framework">
        <div class="section_name">Chimpanzee Face Recognition</div>
        <div class="section_content">
            <p>
                Face recognition is a crucial tool for studying chimpanzees as it enables researchers to identify and track individual animals over time, facilitating long-term behavioral studies, population monitoring, and conservation efforts. By automating the identification process, it reduces the reliance on manual labeling, saving time and resources while improving accuracy.
            </p>
            <p>
                Face recognition involves several tasks, including <strong>verification</strong>, where the objective is to determine whether two chimpanzee face images belong to the same individual.
            </p>
            <div class="img background_white_squared" style="border: none; padding: 0em 0; width: 70%;">
                <img src="./images/chimp_ufe/verification.jpg" alt="Verification example" style="width: 100%;">
                <p style="text-align: center; font-style: italic; margin-top: 0.5em; padding: 0em;">Is it the same individual or not?</p>
            </div>
            <p>
                Another important task is <strong>re-identification</strong>, which involves matching a chimpanzee face image to a gallery of faces, often across different cameras or time frames.
            </p>
            <div class="img background_white_squared" style="border: none; padding: 0em 0;">
                <img src="./images/chimp_ufe/re_identification.jpg" alt="Re-identification example">
                <p style="text-align: center; font-style: italic; margin-top: 0.5em; padding: 0em;">Which individual from the gallery does this query face belong to?</p>
            </div>
            <p>
                Our approach tackles <strong>open</strong>-set face recognition, which means identifying individuals not seen during training. This is a challenging yet essential aspect for real-world applications, as it ensures scalability and adaptability to new data.
            </p>
        </div>
    </div>

    <!-- Camera Traps -->
    <div class="our_framework">
        <div class="section_name">Camera Traps for Biodiversity Monitoring</div>
        <div class="section_content">
            <div class="img background_white_squared" style="border: none; padding: 0em 0;">
                <!-- <img src="./images/chimp_ufe/camera_trap_details.jpg" alt="Camera trap details showing its size, mounting, and night footage"> -->
                <img src="./images/chimp_ufe/camera_trap_on_tree.jpg" alt="Camera trap mounted on a tree" style="width: 41%; border: 2px solid #333; margin-right: 1em;">
                <img src="./images/chimp_ufe/camera_trap_animation.webp" alt="Sample frame from a camera trap clip" style="width: 53.4%; border: 2px solid #333;">
                <p style="text-align: center; font-style: italic; margin-top: 0.5em; padding: 0em;">
                    Camera trap mounted on a tree (left) and sample frame from a camera trap clip (right).
                </p>
            </div>
            <p>
                Biodiversity and wildlife research increasingly depends on small, motion-triggered devices called <strong>camera traps</strong>,
                typically mounted to trees, that capture short video clips or images of animals in their natural habitat. They operate
                both day and night, and are widely accessible, affordable, and scalable — making them ideal for non-invasive monitoring
                to study the behaviour and distribution of wildlife.
            </p>
        </div>
    </div>
    <div class="our_framework">
        <div class="section_name">The Challenge of Identifying Individuals in the Wild</div>
        <div class="section_content">
            <p>
                Unlike animals in captivity or at small research sites, wild populations often include hundreds of
                individuals spread across large areas, making individual identification extremely challenging and slow. Many
                crucial ecological insights rely on knowing which individual is which — from modelling population structure
                to tracking the spread of behaviour and disease.
            </p>
            <p>
                Several chimpanzee face recognition datasets exist, including <a class="intext"
                    href="https://www.robots.ox.ac.uk/~vgg/publications/2019/Schofield19/schofield19.pdf">Bossou</a>, <a
                    class="intext" href="https://dahlian00.github.io/PetFacePage/">PetFace</a>, <a class="intext"
                    href="https://inf-cv.uni-jena.de/home/research/datasets/chimpanzee-faces/">C-Tai and C-Zoo</a>. However, these
                datasets are either collected in captive environments or are relatively small in scale, limiting their utility for
                training robust models. Likewise, traditional supervised approaches require painstakingly labelled identities and
                cannot easily generalise to new populations.
            </p>
        </div>
    </div>
    <div class="our_framework">
        <div class="section_name">Our Approach: Self-Supervised Learning on Camera Trap Footage</div>
        <div class="section_content">
            <p>
                Instead, we propose a <strong>self-supervised</strong> approach that leverages large-scale, easily collected footage from
                camera traps in the wild. By training directly on unlabelled data, our method learns powerful face
                representations without the need for identity labels. This not only enables scalable, non-invasive
                monitoring of chimpanzee populations, but also provides a blueprint for applying similar techniques to other
                species.
            </p>
            <p>
                Such models could be used for downstream tasks like automated estimates of population size and modelling
                spatiotemporal patterns or population structure from large video datasets, opening the door to more scalable
                and impactful wildlife research and conservation efforts globally.
            </p>
        </div>
    </div>

    <!-- Overview Figure -->
    <div class="our_framework">
        <div class="section_name">Pipeline Overview</div>
        <div class="section_content">
            <p>
                Our <strong>Universal Face Embedder</strong> leverages large-scale, unlabelled training data and a
                self-supervised ViT backbone to learn a retrieval-friendly embedding without identity annotations. A
                lightweight face detector and optional tracker form a <em>Data Engine</em> that mines high-quality face
                crops from raw camera-trap videos.
            </p>
            <div class="img background_white_squared" style="border: none; padding: 1em 0 0 0;">
                <img src="./images/chimp_ufe/method.webp" alt="Pre-training feature extractors contrastivelly across time">
            </div>
        </div>
    </div>

    <!-- Model Design -->
    <div class="our_framework">
        <div class="section_name">Model Design</div>
        <div class="section_content">
            <p>
                We train a Vision Transformer (ViT-Small and ViT-Base variants, see <a class="intext" href="https://github.com/v-iashin/ChimpUFE/">code</a> for weights) with the DINOv2 self-distillation
                objective. The model maps each face crop to a <em>d</em>-dimensional representation. During
                inference, open-set identification is performed via <em>k</em>-nearest-neighbour search in the embedding
                space.
            </p>
            <ul>
                <li><b>No identity labels</b> required during training.</li>
                <li>Embeddings are optimised for cosine-similarity retrieval useful for clustering and search tasks.</li>
                <li>ViT-Small variant runs at &gt;100 FPS on a single NVIDIA A4000 GPU.</li>
            </ul>
        </div>
    </div>

    <!-- Data Engine -->
    <div class="our_framework">
        <div class="section_name">Data Engine</div>
        <div class="section_content">
            <p>
                The Data Engine converts raw camera-trap footage into a large corpus of in-the-wild face crops:
            </p>
            <ol>
                <li><strong>Face Detection:</strong> A task-specific YOLOX model trained on ≈2k annotated frames.</li>
                <li><strong>Bounding-Box Tracking:</strong> ByteTrack associates detections across frames, lengthening
                    trajectories, suppressing false positives, and enabling within-video positives and negatives if needed.</li>
                <li><strong>Quality Filtering:</strong> We filter ~630k high-confidence crops from two in-the-wild camera trap datasets: <a class="intext" href="https://data.bris.ac.uk/data/dataset/1h73erszj3ckn2qjwm4sqmr2wt">PanAf-20K</a> and <a class="intext" href="https://www.tacugama.com/">Loma Mountains</a>.</li>
            </ol>
        </div>
    </div>

    <!-- Datasets -->
    <div class="our_framework">
        <div class="section_name">Datasets</div>
        <div class="section_content">
            <p>
                We <strong>train</strong> on three large in-the-wild datasets of chimpanzee faces, including the camera trap
                datasets
                (PanAf-20K and Loma Mountains) and an additional wild dataset called Bossou-14, which consists of 335k images for 14
                individuals. Identity labels are not used during training.
            </p>

            <p>
                We evaluate our model on two open-set face recognition tasks: <strong>re-identification</strong> (retrieving the
                correct identity for a query image) and <strong>verification</strong> (determining if two images belong to the same
                individual).
                Evaluation is performed on two datasets: <strong>Bossou-9</strong> and <strong>PetFaceChimp</strong>. Bossou-9
                contains 9 identities that are disjoint from the training set, providing a strict open-set test. PetFaceChimp is
                collected in a captive environment, unlike our training data, and thus benchmarks the model's generalisation to new
                domains.
            </p>
            <!-- <table style="margin-top:1.5em;">
                <thead>
                    <tr>
                        <th></th>
                        <th></th>
                        <th colspan="2" style="border-left:0.1em solid black; text-align:center;"><strong>Re-Identification</strong></th>
                        <th style="border-left:0.1em solid black; text-align:center;"><strong>Verification</strong></th>
                    </tr>
                    <tr>
                        <th><strong>Eval Dataset</strong></th>
                        <th><strong>IDs</strong></th>
                        <th class="left_border" style="text-align:center;"><strong>Gallery</strong></th>
                        <th style="text-align:center;"><strong>Queries</strong></th>
                        <th class="left_border" style="text-align:center;"><strong>+/–</strong></th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Bossou-9</td>
                        <td>9</td>
                        <td class="left_border" style="text-align:center;">3 150</td>
                        <td style="text-align:center;">630</td>
                        <td class="left_border" style="text-align:center;">7 749</td>
                    </tr>
                    <tr>
                        <td>
                            <div class="popup">
                                <div class="popup_content" style="font-weight: normal;">
                                    We took the chimpanzee subset of the PetFace dataset, filtered it for near-duplicates, and re-split it for re-identification and verification tasks.
                                </div>
                                <a class="popup_link"  tabindex="0">PetFaceChimp*</a>
                            </div>
                        </td>
                        <td>376</td>
                        <td class="left_border" style="text-align:center;">2 477</td>
                        <td style="text-align:center;">376</td>
                        <td class="left_border" style="text-align:center;">15 205</td>
                    </tr>
                </tbody>
            </table> -->
        </div>
    </div>

    <!-- Results -->
    <div class="our_framework">
        <div class="section_name">Results</div>
        <div class="section_content">
            <p>
                Our study shows that <strong>self-supervised</strong> Vision Transformers trained with DINOv2 on chimpanzee face tracks can serve as
                universal chimpanzee face embedders, <strong>surpassing supervised</strong> baselines on Bossou and approaching their performance on PetFace,
                while outperforming all baselines on verification, without requiring identity labels.
            </p>
            <table>
                <thead>
                    <tr>
                        <th></th>
                        <th></th>
                        <th></th>
                        <th colspan="2" style="text-align:center;"><strong>Re-ID (Accuracy, %)</strong></th>
                        <th colspan="2" style="text-align:center;"><strong>Verification (AUC-ROC)</strong></th>
                    </tr>
                    <tr>
                        <th><strong>Method</strong></th>
                        <th><strong>Res.</strong></th>
                        <th><strong>Params</strong></th>
                        <th><strong>Bossou-9</strong></th>
                        <th>
                            <div class="popup">
                                <div class="popup_content" style="font-weight: normal;">
                                    We took the chimpanzee subset of the PetFace dataset, filtered it for near-duplicates, and re-split it for re-identification and verification tasks.
                                </div>
                                <a class="popup_link"  tabindex="0">PetFaceC*</a>
                            </div>
                        </th>
                        <th><strong>Bossou-9</strong></th>
                        <th>
                            <div class="popup">
                                <div class="popup_content" style="font-weight: normal;">
                                    We took the chimpanzee subset of the PetFace dataset, filtered it for near-duplicates, and re-split it for re-identification and verification tasks.
                                </div>
                                <a class="popup_link"  tabindex="0">PetFaceC*</a>
                            </div>
                        </th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td colspan="7" style="text-align: left; font-style: italic;">Supervised methods</td>
                    </tr>
                    <tr>
                        <td>MegaDescriptor</td>
                        <td>384x</td>
                        <td>229M</td>
                        <td>51.1</td>
                        <td>26.2</td>
                        <td>64.5</td>
                        <td>71.7</td>
                    </tr>
                    <tr>
                        <td>MiewID-msv3</td>
                        <td>440x</td>
                        <td>51M</td>
                        <td>56.7</td>
                        <td><strong>49.4</strong></td>
                        <td>61.5</td>
                        <td>74.6</td>
                    </tr>
                    <tr>
                        <td colspan="7" style="text-align: left; font-style: italic; padding-top: 1em;">Self-supervised methods</td>
                    </tr>
                    <tr>
                        <td>Ours </td>
                        <td>224x</td>
                        <td>22M</td>
                        <td>78.1</td>
                        <td>39.3</td>
                        <td>71.8</td>
                        <td>73.7</td>
                    </tr>
                    <tr>
                        <td>Ours </td>
                        <td>224x</td>
                        <td>87M</td>
                        <td><strong>82.2</strong></td>
                        <td>45.9</td>
                        <td><strong>74.2</strong></td>
                        <td><strong>76.3</strong></td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>


    <!-- Acknowledgements -->
    <div class="our_framework">
        <div class="section_name">Acknowledgements</div>
        <div class="section_content">
            <p>
                This research was funded by EPSRC Programme Grant
                <a class="intext" href="https://www.robots.ox.ac.uk/~vgg/projects/visualai/">VisualAI</a>
                EP/T028572/1. We thank the Guinean
                authorities (DGERSIT &amp; IREB), Tetsuro Matsuzawa, Kyoto University, and contributors to the Bossou
                dataset, <a class="intext" href="https://www.tacugama.com/">Tacugama Chimpanzee Sanctuary (TCS)</a>, local authorities,
                and field staff for access to Loma Mountains National Park
                camera-trap data and support with data sharing and conservation.
            </p>
        </div>
    </div>

</body>

</html>
