<!DOCTYPE html>

<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üë®üèº‚Äçüíª</text></svg>">
    <meta name="description" content="Vladimir Iashin's personal website">
    <title> Vladimir Iashin </title>
  </head>

  <style>
    body {
      font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
      font-size: 18px;
      line-height: 1.5;
      margin: 0 auto;
      max-width: 70ex;
      padding: 0 20px 2em;
      background-color: #f7f7f7;
      color: #222222;
    }
    /* links */
    a {
      color: #0077ff;
      text-decoration: none;
      transition: color 0.2s ease;
    }
    a:hover {
      color: #0055ff;
      text-decoration: underline;
    }
    a:active {
      color: #0000ff;
    }
    /* headers */
    h1, h2 {
      line-height: 1.2;
      margin-top: 1.5em;
      margin-bottom: 0.5em;
    }
    h1 {
      font-size: 2.5em;
    }
    h2 {
      font-size: 2em;
    }
    h3 {
      font-size: 1.5em;
    }
    /* paragraphs */
    p {
      margin-top: 1.5em;
      margin-bottom: 1.5em;
    }

  </style>

  <body>
    <h1> Vladimir Iashin <span style="font-size: 0.4em; color: rgb(186, 186, 186);"><br>/vla-DEE-meer/ /YA-sheen/ </span>  </h1>

    <p>
      I'm a deep learning researcher working as a postdoc at the Visual Geometry Group
      (<a href="https://www.robots.ox.ac.uk/~vgg/">VGG</a>)
      in the University of Oxford.
    </p>

    <p>
      My research is focused on figuring out how to make computers understand videos with a mix of visuals,
      sound, and text.
      I build neural nets that bring all these pieces together.
    </p>

    <p>
      I completed my PhD (with distinction) in EECS at <a href="https://www.tuni.fi/en">Tampere University</a>, supervised by
      <a href="https://esa.rahtu.fi/">Esa Rahtu</a>.
      I hold an MSc (with distinction) in Applied Mathematics and Computer Science, along with a BSc in Economics from
      <a href="https://www.hse.ru/en/">HSE University</a>.
    </p>

    <p>
      In my free time, I enjoy hitting the gym, and searching for terroir in coffee.
    </p>

    <p>
      [<a href="https://scholar.google.com/citations?user=rh8_sSkAAAAJ">Google Scholar</a>] ‚Ä¢
      [<a href="https://github.com/v-iashin">GitHub</a>] ‚Ä¢
      [<a href="https://twitter.com/_iashin">X/Twitter</a>] ‚Ä¢
      [<a href="https://www.linkedin.com/in/vladimir-iashin/">LinkedIn</a>]
    </p>

    <h2> Selected Publications </h2>

      <p>
        <b>Synchformer: Efficient Synchronization from Sparse Cues</b> <br>
        <i>Vladimir Iashin</i>, Weidi Xie, Esa Rahtu, and Andrew Zisserman <br>
        ICASSP, 2024 <br>
        [<a href="https://www.robots.ox.ac.uk/~vgg/research/synchformer/">Project Page</a>] ‚Ä¢
        [<a href="https://github.com/v-iashin/Synchformer">Code</a>] ‚Ä¢
        [<a href="https://arxiv.org/abs/2401.16423">Paper</a>]
      </p>

      <!-- <p>
        <b>Multi-modal Video Content Understanding</b> <br>
        Vladimir Iashin <br>
        Tampere University Dissertations, 2023 <br>
        [<a href="https://trepo.tuni.fi/bitstream/handle/10024/147432/978-952-03-2872-6.pdf?sequence=2&isAllowed=y">Thesis</a>] ‚Ä¢
        [<a href="https://www.youtube.com/watch?v=IXteaQ5yPrU">Presentation</a>]
      </p> -->

      <p>
        <b>Sparse in Space and Time: Audio-visual Synchronisation with Trainable Selectors</b> <br>
        <i>Vladimir Iashin</i>, Weidi Xie, Esa Rahtu, and Andrew Zisserman <br>
        BMVC, 2022 (Spotlight) <br>
        [<a href="SparseSync.html">Project Page</a>] ‚Ä¢
        [<a href="https://github.com/v-iashin/SparseSync">Code</a>] ‚Ä¢
        [<a href="https://arxiv.org/abs/2210.07055">Paper</a>] ‚Ä¢
        [<a href="https://www.youtube.com/watch?v=q-232MJo0_E">Presentation</a>]
      </p>

      <p>
        <b>Taming Visually Guided Sound Generation</b> <br>
        <i>Vladimir Iashin</i> and Esa Rahtu <br>
        BMVC, 2021 (Oral) <br>
        [<a href="SpecVQGAN.html">Project Page</a>] ‚Ä¢
        [<a href="https://github.com/v-iashin/SpecVQGAN">Code</a>] ‚Ä¢
        [<a href="https://arxiv.org/abs/2110.08791">Paper</a>] ‚Ä¢
        [<a href="https://www.youtube.com/watch?v=Bucb3nAa398">Presentation</a>]
      </p>

      <p>
        <b>A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer</b> <br>
        <i>Vladimir Iashin</i> and Esa Rahtu <br>
        BMVC, 2020 <br>
        [<a href="bmt.html">Project Page</a>] ‚Ä¢
        [<a href="https://github.com/v-iashin/BMT">Code</a>] ‚Ä¢
        [<a href="https://arxiv.org/abs/2005.08271">Paper</a>] ‚Ä¢
        [<a href="https://www.youtube.com/watch?v=C4zYVIqGDVQ">Presentation</a>]
      </p>

      <p>
        <b>Multi-modal Dense Video Captioning</b> <br>
        <i>Vladimir Iashin</i> and Esa Rahtu <br>
        CVPR Workshops, 2020 <br>
        [<a href="mdvc.html">Project Page</a>] ‚Ä¢
        [<a href="https://github.com/v-iashin/mdvc">Code</a>] ‚Ä¢
        [<a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w56/Iashin_Multi-Modal_Dense_Video_Captioning_CVPRW_2020_paper.html">Paper</a>] ‚Ä¢
        [<a href="https://www.youtube.com/watch?v=0Vmx_gzP1bM">Presentation</a>]
      </p>

      <h2> Community Service </h2>

        <p>
          <b>Reviewer</b> <br>
          AAAI 2023, CVPR 2022, ICCV 2021, TPAMI 2020
        </p>

      <h2> Misc </h2>

        <p>
          <b><a href="https://v-iashin.github.io/video_features/">Video Features</a></b> <br>
          Enables seamless feature extraction and optical flow frame extraction from raw videos
          with multi-GPU acceleration through a user-friendly and flexible API.
        </p>

        <p>
          <b><a href="https://v-iashin.github.io/detector"> Object Detector </a></b> <br>
          Discover the contents of your uploaded image effortlessly. <br>
          The detector is based on YOLOv3 and implemented in PyTorch.
          The computation is done on <s>a cloud server which runs a Flask application (see the Note)</s>
          HuggingFace Spaces with Gradio UI. <br>
          [<a href="https://github.com/v-iashin/WebsiteYOLO/">Code</a>] ‚Ä¢
          [<a href="https://v-iashin.github.io/how_did_you_build_your_detector"><s>Note</s></a>]
        </p>

        <p>
          <b> <a href="https://tuni-itc.github.io/wiki/">ITC Wiki</a> </b> <br>
          During my PhD studies, I organized a crowd-sourced wiki which helps with common internal questions
          such as how to set up remote access to office GPU machines.
        </p>

        <p>
          <b><a href="ide_customization.html">IDE Customization</a></b> <br>
          A note about VSCode customization.
        </p>

    </body>

    <!-- Prefetching the hidden images for snappier hovers-->
    <!-- <img src="./images/sync_out.png" style="display: none;">
    <img src="./images/specvqgan_out.svg" style="display: none;">
    <img src="./images/typical_russian_day_orig.jpeg" style="display: none;">
    <img src="./images/mdvc.gif" style="display: none;">
    <img src="./images/video_features/vid_feats.gif" style="display: none;">
    <img src="./images/enc_dec.png" style="display: none;"> -->
</html>
