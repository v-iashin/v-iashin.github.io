<!DOCTYPE html>

<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üë®üèº‚Äçüíª</text></svg>">
    <meta name="description" content="Vladimir Iashin's personal website">
    <title> Vladimir Iashin </title>
  </head>

  <style>
    body {
      font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
      font-size: 18px;
      line-height: 1.5;
      margin: 0 auto;
      max-width: 70ex;
      padding: 0 20px 2em;
      background-color: #f7f7f7;
      color: #222222;
    }
    /* links */
    a {
      color: #0077ff;
      text-decoration: none;
      transition: color 0.2s ease;
    }
    a:hover {
      color: #0055ff;
      text-decoration: underline;
    }
    a:active {
      color: #0000ff;
    }
    /* headers */
    h1, h2 {
      line-height: 1.2;
      margin-top: 1.5em;
      margin-bottom: 0.5em;
    }
    h1 {
      font-size: 2.5em;
    }
    h2 {
      font-size: 2em;
    }
    h3 {
      font-size: 1.5em;
    }
    /* paragraphs */
    p {
      margin-top: 1.5em;
      margin-bottom: 1.5em;
    }

  </style>

  <body>
    <h1> Vladimir Iashin </h1>

    <p>
      I am a deep learning researcher and Ph.D. candidate who specializes in designing neural nets for video content understanding.
      I enjoy working at the intersection of deep learning, computer vision, sound, and natural language processing.
    </p>

    <p>
      [<a href="https://www.linkedin.com/in/vladimir-iashin/">LinkedIn</a>] ‚Ä¢
      [<a href="https://scholar.google.com/citations?user=rh8_sSkAAAAJ">Google Scholar</a>] ‚Ä¢
      [<a href="https://github.com/v-iashin">GitHub</a>] ‚Ä¢
      [<a href="https://twitter.com/_iashin">Twitter</a>]
    </p>

    <h2> Publications </h2>

      <p>
        <b>Sparse in Space and Time: Audio-visual Synchronisation with Trainable Selectors</b> <br>
        <i>Vladimir Iashin</i>, Weidi Xie, Esa Rahtu, and Andrew Zisserman <br>
        BMVC, 2022 (Spotlight) <br>
        [<a href="SparseSync.html">Project Page</a>] ‚Ä¢
        [<a href="https://github.com/v-iashin/SparseSync">Code</a>] ‚Ä¢
        [<a href="https://arxiv.org/abs/2210.07055">Paper</a>] ‚Ä¢
        [<a href="https://www.youtube.com/watch?v=q-232MJo0_E">Presentation</a>]
      </p>

      <p>
        <b>Taming Visually Guided Sound Generation</b> <br>
        <i>Vladimir Iashin</i> and Esa Rahtu <br>
        BMVC, 2021 (Oral) <br>
        [<a href="SpecVQGAN.html">Project Page</a>] ‚Ä¢
        [<a href="https://github.com/v-iashin/SpecVQGAN">Code</a>] ‚Ä¢
        [<a href="https://arxiv.org/abs/2110.08791">Paper</a>] ‚Ä¢
        [<a href="https://www.youtube.com/watch?v=Bucb3nAa398">Presentation</a>]
      </p>

      <p>
        <b>A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer</b> <br>
        <i>Vladimir Iashin</i> and Esa Rahtu <br>
        BMVC, 2020 <br>
        [<a href="bmt.html">Project Page</a>] ‚Ä¢
        [<a href="https://github.com/v-iashin/BMT">Code</a>] ‚Ä¢
        [<a href="https://arxiv.org/abs/2005.08271">Paper</a>] ‚Ä¢
        [<a href="https://www.youtube.com/watch?v=C4zYVIqGDVQ">Presentation</a>]
      </p>

      <p>
        <b>Multi-modal Dense Video Captioning</b> <br>
        <i>Vladimir Iashin</i> and Esa Rahtu <br>
        CVPR Workshops, 2020 <br>
        [<a href="mdvc.html">Project Page</a>] ‚Ä¢
        [<a href="https://github.com/v-iashin/mdvc">Code</a>] ‚Ä¢
        [<a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w56/Iashin_Multi-Modal_Dense_Video_Captioning_CVPRW_2020_paper.html">Paper</a>] ‚Ä¢
        [<a href="https://www.youtube.com/watch?v=0Vmx_gzP1bM">Presentation</a>]
      </p>

      <h2> Projects </h2>

        <p>
          <b> Video Features </b> <br>
          Provides an easy and flexible API for feature extraction as well as optical flow frame extraction
          from raw videos allowing multi-GPU acceleration. <br>
          [<a href="https://v-iashin.github.io/video_features/">Documentation</a>] ‚Ä¢
          [<a href="https://github.com/v-iashin/video_features/">Code</a>]
        </p>

        <p>
          <b> Object Detector </b> <br>
          It will tell you what's on the image you uploaded. <br>
          The detector is based on YOLOv3 and implemented in PyTorch.
          The computation is done on a cloud server which runs a Flask application. <br>
          [<a href="https://v-iashin.github.io/detector">Demo</a>] ‚Ä¢
          [<a href="https://github.com/v-iashin/WebsiteYOLO/">Back-end</a>] ‚Ä¢
          [<a href="https://v-iashin.github.io/how_did_you_build_your_detector">Note</a>]
        </p>

      <h2> Notes </h2>

        <p>
          <b>IDE Customization</b> <br>
          A note about VSCode customization ‚öôÔ∏è. It contains a list of cool extensions üß© and settings üéõ that
          I found useful in my daily coding routine. <br>
          [<a href="ide_customization.html">Note</a>]
        </p>

        <p>
          <b> Object Detector as a Web App: How to Build It? </b> <br>
          Engineering notes describing how to build a basic full-stack website app that serves user requests.
          It describes how to design front- and back-ends, rent a server in the cloud, use DNS, and rent a domain name. <br>
          [<a href="https://v-iashin.github.io/how_did_you_build_your_detector">Note</a>]
        </p>

    </body>

    <!-- Prefetching the hidden images for snappier hovers-->
    <!-- <img src="./images/sync_out.png" style="display: none;">
    <img src="./images/specvqgan_out.svg" style="display: none;">
    <img src="./images/typical_russian_day_orig.jpeg" style="display: none;">
    <img src="./images/mdvc.gif" style="display: none;">
    <img src="./images/video_features/vid_feats.gif" style="display: none;">
    <img src="./images/enc_dec.png" style="display: none;"> -->
</html>
