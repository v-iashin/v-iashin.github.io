<!DOCTYPE html>

<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üë®üèº‚Äçüíª</text></svg>">
    <meta name="description" content="Vladimir Iashin's personal website">
    <title> Vladimir Iashin </title>
    <meta name="color-scheme" content="light dark">
    <meta name="theme-color" content="#ffffff" media="(prefers-color-scheme: light)">
    <meta name="theme-color" content="#0b0f19" media="(prefers-color-scheme: dark)">
  </head>

  <style>
    :root {
      --bg: #ffffff;
      --text: #1f2937; /* slate-800 */
      --muted: #6b7280; /* gray-500 */
      --link: #2563eb; /* blue-600 */
      --link-hover: #1d4ed8; /* blue-700 */
      --border: #e5e7eb; /* gray-200 */
    }

    @media (prefers-color-scheme: dark) {
      :root {
        --bg: #0b0f19; /* near-navy */
        --text: #e5e7eb; /* gray-200 */
        --muted: #9aa4b2; /* slate-400 */
        --link: #93c5fd; /* blue-300 */
        --link-hover: #bfdbfe; /* blue-200 */
        --border: #1f2937; /* slate-800 */
      }
    }

    html {
      -webkit-text-size-adjust: 100%;
    }

    body {
      font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
      font-size: 18px;
      line-height: 1.7;
      margin: 0 auto;
      max-width: 70ch;
      padding: 2.5rem 20px 3rem;
      background-color: var(--bg);
      color: var(--text);
      text-rendering: optimizeLegibility;
      -webkit-font-smoothing: antialiased;
    }

    /* headers */
    h1, h2, h3 {
      line-height: 1.25;
      margin-top: 1.75em;
      margin-bottom: 0.5em;
      letter-spacing: -0.01em;
    }
    h1 { font-size: clamp(2rem, 1.5rem + 1.5vw, 2.5rem); }
    h2 { font-size: clamp(1.5rem, 1.2rem + 1vw, 2rem); }
    h3 { font-size: 1.25rem; }

    /* paragraphs */
    p {
      margin: 1.1em 0;
    }

    /* links */
    a {
      color: var(--link);
      text-decoration: underline;
      text-decoration-color: color-mix(in oklab, var(--link) 35%, transparent);
      text-underline-offset: 3px;
      text-decoration-thickness: 1px;
      transition: color 0.15s ease, text-decoration-color 0.15s ease;
    }
    a:hover {
      color: var(--link-hover);
      text-decoration-color: var(--link-hover);
    }
    a:active { color: var(--link-hover); }
    a:focus-visible {
      outline: 2px solid color-mix(in oklab, var(--link) 50%, transparent);
      outline-offset: 2px;
      border-radius: 3px;
    }

    ::selection {
      background: color-mix(in oklab, var(--link) 25%, transparent);
    }

    /* Slightly mute the phonetic hint under the name */
    h1 span {
      color: var(--muted) !important;
    }

    /* Gentle spacing after section headings */
    h2 + p { margin-top: 0.75em; }
  </style>

  <body>
    <h1> Vladimir Iashin <span style="font-size: 0.4em; color: rgb(186, 186, 186);"><br>/vla-DEE-meer/ /YA-sheen/ </span>  </h1>

    <p>
      I'm a deep learning researcher working as a postdoc at the Visual Geometry Group
      (<a href="https://www.robots.ox.ac.uk/~vgg/">VGG</a>)
      in the University of Oxford.
    </p>

    <p>
      My research is focused on figuring out how to make computers understand videos with a mix of visuals,
      sound, and text.
      I build neural nets that bring all these pieces together.
    </p>

    <p>
      I completed my PhD (with distinction) in EECS at <a href="https://www.tuni.fi/en">Tampere University</a>, supervised by
      <a href="https://esa.rahtu.fi/">Esa Rahtu</a>.
      I hold an MSc (with distinction) in Applied Mathematics and Computer Science, along with a BSc in Economics from
      <a href="https://www.hse.ru/en/">HSE University</a>.
    </p>

    <p>
      In my free time, I enjoy hitting the gym, and searching for terroir in coffee.
    </p>

    <p>
      [<a href="https://scholar.google.com/citations?user=rh8_sSkAAAAJ">Google Scholar</a>] ‚Ä¢
      [<a href="https://github.com/v-iashin">GitHub</a>] ‚Ä¢
      [<a href="https://twitter.com/_iashin">X/Twitter</a>] ‚Ä¢
      [<a href="https://www.linkedin.com/in/vladimir-iashin/">LinkedIn</a>]
    </p>

    <h2> Selected Publications </h2>

      <p>
        <b>Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder</b> <br>
        <i>Vladimir Iashin</i>, Horace Lee, Dan Schofield, and Andrew Zisserman <br>
        Computer Vision for Ecological and Biodiversity Monitoring Workshop, ICIP, 2025 <br>
        [<a href="https://www.robots.ox.ac.uk/~vgg/research/ChimpUFE/">Project Page</a>] ‚Ä¢
        [<a href="https://github.com/v-iashin/ChimpUFE">Code</a>] ‚Ä¢
        [<a href="https://arxiv.org/abs/2507.10552">Paper</a>]
      </p>

      <p>
        <b>Temporally Aligned Audio for Video with Autoregression</b> <br>
        Ilpo Viertola, <i>Vladimir Iashin</i>, Esa Rahtu <br>
        ICASSP, 2025 (Oral) <br>
        [<a href="https://v-aura.notion.site/">Project Page</a>] ‚Ä¢
        [<a href="https://github.com/ilpoviertola/V-AURA">Code</a>] ‚Ä¢
        [<a href="https://arxiv.org/abs/2409.13689">Paper</a>]
      </p>

      <p>
        <b>Synchformer: Efficient Synchronization from Sparse Cues</b> <br>
        <i>Vladimir Iashin</i>, Weidi Xie, Esa Rahtu, and Andrew Zisserman <br>
        ICASSP, 2024 <br>
        [<a href="https://www.robots.ox.ac.uk/~vgg/research/synchformer/">Project Page</a>] ‚Ä¢
        [<a href="https://github.com/v-iashin/Synchformer">Code</a>] ‚Ä¢
        [<a href="https://arxiv.org/abs/2401.16423">Paper</a>]
      </p>

      <!-- <p>
        <b>Multi-modal Video Content Understanding</b> <br>
        Vladimir Iashin <br>
        Tampere University Dissertations, 2023 <br>
        [<a href="https://trepo.tuni.fi/bitstream/handle/10024/147432/978-952-03-2872-6.pdf?sequence=2&isAllowed=y">Thesis</a>] ‚Ä¢
        [<a href="https://www.youtube.com/watch?v=IXteaQ5yPrU">Presentation</a>]
      </p> -->

      <p>
        <b>Sparse in Space and Time: Audio-visual Synchronisation with Trainable Selectors</b> <br>
        <i>Vladimir Iashin</i>, Weidi Xie, Esa Rahtu, and Andrew Zisserman <br>
        BMVC, 2022 (Spotlight) <br>
        [<a href="SparseSync.html">Project Page</a>] ‚Ä¢
        [<a href="https://github.com/v-iashin/SparseSync">Code</a>] ‚Ä¢
        [<a href="https://arxiv.org/abs/2210.07055">Paper</a>] ‚Ä¢
        [<a href="https://www.youtube.com/watch?v=q-232MJo0_E">Presentation</a>]
      </p>

      <p>
        <b>Taming Visually Guided Sound Generation</b> <br>
        <i>Vladimir Iashin</i> and Esa Rahtu <br>
        BMVC, 2021 (Oral) <br>
        [<a href="SpecVQGAN.html">Project Page</a>] ‚Ä¢
        [<a href="https://github.com/v-iashin/SpecVQGAN">Code</a>] ‚Ä¢
        [<a href="https://arxiv.org/abs/2110.08791">Paper</a>] ‚Ä¢
        [<a href="https://www.youtube.com/watch?v=Bucb3nAa398">Presentation</a>]
      </p>

      <p>
        <b>A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer</b> <br>
        <i>Vladimir Iashin</i> and Esa Rahtu <br>
        BMVC, 2020 <br>
        [<a href="bmt.html">Project Page</a>] ‚Ä¢
        [<a href="https://github.com/v-iashin/BMT">Code</a>] ‚Ä¢
        [<a href="https://arxiv.org/abs/2005.08271">Paper</a>] ‚Ä¢
        [<a href="https://www.youtube.com/watch?v=C4zYVIqGDVQ">Presentation</a>]
      </p>

      <p>
        <b>Multi-modal Dense Video Captioning</b> <br>
        <i>Vladimir Iashin</i> and Esa Rahtu <br>
        Multimodal Learning Workshop, CVPR, 2020 <br>
        [<a href="mdvc.html">Project Page</a>] ‚Ä¢
        [<a href="https://github.com/v-iashin/mdvc">Code</a>] ‚Ä¢
        [<a href="http://openaccess.thecvf.com/content_CVPRW_2020/html/w56/Iashin_Multi-Modal_Dense_Video_Captioning_CVPRW_2020_paper.html">Paper</a>] ‚Ä¢
        [<a href="https://www.youtube.com/watch?v=0Vmx_gzP1bM">Presentation</a>]
      </p>

      <h2> Community Service </h2>

        <p>
          <b>Reviewer</b> <br>
          AAAI 2023, CVPR 2022, ICCV 2021, TPAMI 2020, and conference workshops in 2024 & 2021
        </p>

      <h2> Misc </h2>

        <p>
          <b><a href="https://v-iashin.github.io/video_features/">Video Features</a></b> <br>
          Enables seamless feature extraction and optical flow frame extraction from raw videos
          with multi-GPU acceleration through a user-friendly and flexible API.
        </p>

        <p>
          <b><a href="https://v-iashin.github.io/detector"> Object Detector </a></b> <br>
          Discover the contents of your uploaded image effortlessly. <br>
          The detector is based on YOLOv3 and implemented in PyTorch.
          The computation is done on <s>a cloud server which runs a Flask application (see the Note)</s>
          HuggingFace Spaces with Gradio UI. <br>
          [<a href="https://github.com/v-iashin/WebsiteYOLO/">Code</a>] ‚Ä¢
          [<a href="https://v-iashin.github.io/how_did_you_build_your_detector"><s>Note</s></a>]
        </p>

        <p>
          <b> <a href="https://tuni-itc.github.io/wiki/">ITC Wiki</a> </b> <br>
          During my PhD studies, I organized a crowd-sourced wiki which helps with common internal questions
          such as how to set up remote access to office GPU machines.
        </p>

        <p>
          <b><a href="ide_customization.html">IDE Customization</a></b> <br>
          A note about VSCode customization.
        </p>

    </body>

    <!-- Prefetching the hidden images for snappier hovers-->
    <!-- <img src="./images/sync_out.png" style="display: none;">
    <img src="./images/specvqgan_out.svg" style="display: none;">
    <img src="./images/typical_russian_day_orig.jpeg" style="display: none;">
    <img src="./images/mdvc.gif" style="display: none;">
    <img src="./images/video_features/vid_feats.gif" style="display: none;">
    <img src="./images/enc_dec.png" style="display: none;"> -->
</html>
